{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4170e259",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network with Back Propagation\n",
    "\n",
    "The objective of this notebook is to implement smartly and scalably a fully connected neural network that will be trained by means of a gradient descent back propagation algorithm. Afterwards I will test some simple networks on small data sets and try to run a regression analysis predicing milage per galon (mpg) based on the mtcars data set.\n",
    "\n",
    "* Get the mtcars data, bulid features and also divide into train and test. \n",
    "* Make some simple linearely separable and XOR data test cases. \n",
    "* Create classes and methods for marticular modules - linear and activation of the neural network\n",
    "* Create a neural network class which is build out of modules and has a training method. \n",
    "* Run the simple test and train the network on mtcars and see if we are getting any reasonable results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea6872",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c8bd3",
   "metadata": {},
   "source": [
    "Get mtcars data and do feature preparation for a regression problem. The NN network will try to learn mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "1d7bedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "    takes path and returns a pnadas data frame object with the data from file under path\n",
    "    '''\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for row in csv.DictReader(f, delimiter='\\t'):\n",
    "            data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def one_hot(x):\n",
    "    '''\n",
    "    @param x : pandas series object\n",
    "    returns a data frame with the original series and the one hot encoded fields\n",
    "    '''\n",
    "    \n",
    "    new_frame = pd.DataFrame(x)\n",
    "    for val in x.unique():\n",
    "        _name = str(int(val))\n",
    "        new_frame[x.name + _name] = new_frame.apply(lambda col: 1 if col[x.name] == val else 0, axis=1)\n",
    "    return new_frame.iloc[:,1:]\n",
    "        \n",
    "    \n",
    "def standardize(x):\n",
    "    '''\n",
    "    @param x : string holding the name of the field to be standarized\n",
    "    '''\n",
    "    z = x.astype('float')\n",
    "    result = np.array((z - np.mean(z))/np.std(z))\n",
    "\n",
    "    return pd.DataFrame(result, columns=[x.name])\n",
    "\n",
    "\n",
    "def make_features(feature_list):\n",
    "    '''\n",
    "    @param feature_list: a list of tuples, first entry is a pandas series and the next one is a string with \n",
    "    onehot or standardize. \n",
    "    '''\n",
    "    new_features = []\n",
    "    for f,ftype in feature_list:\n",
    "        if ftype == 'onehot':\n",
    "            new_features.append(one_hot(f))\n",
    "        elif ftype == 'standardize':\n",
    "            new_features.append(standardize(f))\n",
    "        else:\n",
    "            new_features.append(f.astype('float'))\n",
    "        \n",
    "    return pd.concat(new_features, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "auto_data = load_data('../code_and_data_for_hw05/auto-mpg-regression.tsv')\n",
    "\n",
    "\n",
    "features = [\n",
    "            (auto_data.cylinders, 'onehot'),\n",
    "            (auto_data.displacement, 'standardize'),\n",
    "            (auto_data.horsepower, 'standardize'),\n",
    "            (auto_data.weight, 'standardize'),\n",
    "            (auto_data.acceleration, 'standardize'),\n",
    "            (auto_data.origin, 'onehot'),\n",
    "            (auto_data.mpg, 'standardize')\n",
    "            ]\n",
    "\n",
    "\n",
    "# Keep this for future reference:\n",
    "mean_mpg, sigma_mpg = auto_data['mpg'].astype('float').mean(), auto_data['mpg'].astype('float').std() \n",
    "\n",
    "\n",
    "auto_data_ = make_features(features)\n",
    "\n",
    "\n",
    "\n",
    "# Transpose  data and divide into training and testing sets:\n",
    "X = (auto_data_.T).iloc[:-1, :]\n",
    "Y = (auto_data_.T).iloc[[-1], :]\n",
    "\n",
    "# Divide at random into test and train. The test data set will be approzimately 20% of the entire data set\n",
    "test_ind = np.random.choice(range(X.shape[1]), int(0.2 * X.shape[1]), replace = False)\n",
    "train_ind = np.array(list(set(range(X.shape[1]))- set(test_ind)))\n",
    "\n",
    "X_test, Y_test = np.array(X.iloc[:, test_ind]), np.array(Y.iloc[:, test_ind])\n",
    "X_train, Y_train = np.array(X.iloc[:, train_ind]), np.array(Y.iloc[:, train_ind])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026af4a6",
   "metadata": {},
   "source": [
    "Build versy simple data set that is one dimensional and linearly separable. In fact values above 2 are classified as 1 and values below 2 as zero.  We want to see if the network can classify correctly. Thise is actually an implementation test case. If all is good, the network should be able to handle this classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "id": "d596074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple_1D = np.array([[1.4, -1.5, -0.9, 2.7, 4.1, 5.0 ], [1,1,1,1,1,1]])\n",
    "Y_simple_1D = np.array([[0,0,0,1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36b447",
   "metadata": {},
   "source": [
    "Same as above, but this time we go for two dimensions. Poinst for which $x<0 \\land y>0$ will be classified as 0 and points for which $x>0 \\land y<0$ as 1. This is another implementation test indeed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "922ea090",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple_2D = np.array([[0.4, -1.5, -0.9, 0.9, 1.2, -1.2, 3.1], \n",
    "                        [0.5, -1, -0.4, 1.4, 0.7, -0.5, 1.0] , \n",
    "                        [1, 1, 1, 1, 1, 1, 1]])\n",
    "Y_simple_2D = np.array([[1,0,0,1,1,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d104ebd",
   "metadata": {},
   "source": [
    "And the last imlementaton test is the XOR test. Here we check if the network can learn the XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "4a8a2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xor = np.array([[0,0, 1, 1], [0, 1, 0, 1], [1, 1, 1, 1]])\n",
    "Y_xor = np.array([[0, 1, 1, 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeca971",
   "metadata": {},
   "source": [
    "### Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6f616",
   "metadata": {},
   "source": [
    "Implementation of the linear module with forward and backward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "ccde71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, m,n):\n",
    "        '''\n",
    "            This is the linear part that takes in m inputs, multiplies by weights and produces n outputs\n",
    "            m - number of inputs\n",
    "            n - number of outputs\n",
    "        '''\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.W = np.random.normal(loc=0.0, scale=3 * m ** (-.5), size=(n,m)).reshape((self.n, self.m))\n",
    "        self.W[[n-1],:] = np.zeros((1,self.m))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "            X are the inputs. The forward method will produce and save activations and also return them\n",
    "        '''\n",
    "        assert X.shape[0] == self.m\n",
    "        self.A = X  # if self is layer l, then self.A is l-1 layer, the inputs to this module\n",
    "        self.Z  = np.matmul(self.W, X)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dLdZ):\n",
    "        '''\n",
    "            dLdZ is passed from the activation layer. We can compyte dLdA = dLdZ * dZdA, but dZdA = W\n",
    "            and pass it to the previous activation layer. \n",
    "            knowing dLdZ, we compute dLdW = dLdZ * dZdW = dLdZ * A(l-1)  and store it\n",
    "        '''\n",
    "        self.dLdA = np.matmul((self.W).T, dLdZ)\n",
    "        self.dLdW = np.matmul(self.A, dLdZ.T).T\n",
    "        return self.dLdA\n",
    "    \n",
    "    def update(self, lrate):\n",
    "        '''\n",
    "            just update weights \n",
    "        '''\n",
    "        assert self.W.shape == self.dLdW.shape\n",
    "        self.W = self.W - lrate * self.dLdW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4f5e0",
   "metadata": {},
   "source": [
    "Implementation of the ReLU, sigmoid and identity activations with forward and backward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "id": "7831c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "            Take an X as input and apply ReLU elementwise\n",
    "        '''\n",
    "        def ReLU(x):\n",
    "            return x if x>=0 else 0\n",
    "        self.A = X\n",
    "        self.Z = np.vectorize(lambda x: ReLU(x))(self.A)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        '''\n",
    "            Takes dLdA, where dA means with respect to the output of the module.  \n",
    "            Returns dLdZ (Z is the input to the module). dLdZ = dLdA * dAdZ , but dAdZ is ReLU_grad\n",
    "        '''\n",
    "        def ReLU_grad(x):\n",
    "            return 1 if x>=0 else 0\n",
    "        return dLdA * np.vectorize(lambda x: ReLU_grad(x))(self.A) \n",
    "\n",
    "\n",
    "class Sigmoid(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "            Take an X as input and apply ReLU elementwise\n",
    "        '''\n",
    "        def sigmoid(x):\n",
    "            return 1/(1+exp(-x))\n",
    "        self.A = X\n",
    "        self.Z = np.vectorize(lambda x: sigmoid(x))(self.A)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        '''\n",
    "            Takes dLdA, where dA means with respect to the output of the module.  \n",
    "            Returns dLdZ (Z is the input to the module). dLdZ = dLdA * dAdZ , but dAdZ is ReLU_grad\n",
    "        '''\n",
    "        def sigmoid(x):\n",
    "            return 1/(1+exp(-x))\n",
    "\n",
    "        def sigmoid_grad(x):\n",
    "            return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "        return dLdA * np.vectorize(lambda x: sigmoid_grad(x))(self.A) \n",
    "\n",
    "    \n",
    "class Identity(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "            Take an X as input and apply ReLU elementwise\n",
    "        '''\n",
    "        self.A = X\n",
    "        self.Z = X\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dLdA):\n",
    "        '''\n",
    "            Takes dLdA, where dA means with respect to the output of the module.  \n",
    "            Returns dLdZ (Z is the input to the module). dLdZ = dLdA * dAdZ , but dAdZ is ReLU_grad\n",
    "        '''\n",
    "        return dLdA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d7cc3",
   "metadata": {},
   "source": [
    "Implementation of quadratic and negative log likelyhood loss modules, with loss and loss_grad methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "id": "93596677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quadratic_loss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def loss(self, Ypred, Y):\n",
    "        '''\n",
    "            simple quadratic loss. In general this should be a scalar\n",
    "        '''\n",
    "        return ((Ypred  - Y)**2)#.sum(axis=1, keepdims=True)/(Ypred.shape[1])\n",
    "    \n",
    "    def loss_grad(self, Ypred, Y):\n",
    "        '''\n",
    "            quadratic loss gradient. In general this should be a scalar\n",
    "        '''\n",
    "        self.A = Ypred\n",
    "        self.dLdA = 2*(Ypred - Y)#.sum(axis=1, keepdims=True)/(Ypred.shape[1])\n",
    "        return self.dLdA\n",
    "\n",
    "    \n",
    "class NLL_loss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def loss(self, Ypred, Y):\n",
    "        '''\n",
    "            simple quadratic loss. In general this should be a scalar\n",
    "        '''\n",
    "\n",
    "        return - Y*np.log(Ypred) - (1-Y)*np.log(1 - Ypred)\n",
    "    \n",
    "    def loss_grad(self, Ypred, Y):\n",
    "        '''\n",
    "            quadratic loss gradient. In general this should be a scalar\n",
    "        '''\n",
    "        \n",
    "        self.A = Ypred\n",
    "        self.dLdA = - Y*(1 / Ypred) + (1 - Y)*(1/(1 - Ypred))#.sum(axis=1, keepdims=True)/(Ypred.shape[1])\n",
    "        return self.dLdA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeccceb",
   "metadata": {},
   "source": [
    "Flinally comes the implementation of the entire network with forward pass, backward pass and a train method that uses a stochastic gradient descent to train the netowrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "fcaaf037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    def __init__(self, modules, loss_module):\n",
    "        '''\n",
    "            modules - list of alternating linear and activation modules\n",
    "            loss_module - the final loss module of the network\n",
    "        '''\n",
    "        self.modules = modules\n",
    "        self.loss_module = loss_module\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "            takes input X and makes a forward path returning the last activation, i.e. the output of the nn\n",
    "        '''\n",
    "        L = len(self.modules)\n",
    "        for m in range(L):\n",
    "            Xf = self.modules[m].forward(X)\n",
    "            X = Xf\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dLdA):\n",
    "        '''\n",
    "            takes as input dLdA, where A is the output of the network, and performs a backward pass\n",
    "        '''\n",
    "        L = len(self.modules)\n",
    "        delta = dLdA\n",
    "        for m in range(L-1,-1,-1):\n",
    "            delta_new = self.modules[m].backward(delta)\n",
    "            delta = delta_new\n",
    "    \n",
    "    def train(self, X_train, Y_train, num_iter, lrate):\n",
    "        '''\n",
    "            Train the network using SGD picking one data point from X_train, Y_train\n",
    "        '''\n",
    "        for i in range(num_iter):\n",
    "            t = np.random.randint(X_train.shape[1])\n",
    "\n",
    "            Y_pred = self.forward(X_train[:,[t]])\n",
    "            \n",
    "            delta  = self.loss_module.loss_grad(Y_pred, Y_train[:,[t]])\n",
    "\n",
    "            self.backward(delta)\n",
    "            for module in self.modules:\n",
    "                if 'W' in dir(module):\n",
    "                    module.update(lrate)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e5916",
   "metadata": {},
   "source": [
    "### Testing the nueral network algorithm on some basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "id": "9381c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My test function\n",
    "def test_nn(X, Y, NN, num_iter, lrate):\n",
    "    '''\n",
    "        takes an MM, training set X, labels set Y, num_iter and learning rate as shows how the NN did learning\n",
    "    '''\n",
    "    print('------Ypred---- and--loss--BEFORE---training----\\n')\n",
    "    Ypred = NN.forward(X)\n",
    "    print('prediction: ',  Ypred)\n",
    "    loss = NN.loss_module.loss(Ypred, Y)\n",
    "    print('loss: ', loss.sum())\n",
    "    print('------Ypred---- and--loss--AFTER---training----\\n')\n",
    "\n",
    "    NN.train(X, Y, num_iter, lrate)\n",
    "    Ypred = NN.forward(X)\n",
    "    print('prediction ', Ypred)\n",
    "    loss = NN.loss_module.loss(Ypred, Y)\n",
    "    print('loss: ', loss.sum())\n",
    "\n",
    "    print('---Ypred---and--Y--side--by--side-----\\n ')\n",
    "    print(np.vstack((Ypred, Y)).T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa26538",
   "metadata": {},
   "source": [
    "Just one dimension detect if data is above or below zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "id": "cff2d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Ypred---- and--loss--BEFORE---training----\n",
      "\n",
      "prediction:  [[0.5 0.5 0.5 0.5 0.5 0.5]]\n",
      "loss:  4.1588830833596715\n",
      "------Ypred---- and--loss--AFTER---training----\n",
      "\n",
      "prediction  [[0.16061856 0.00004506 0.00025372 0.89004036 0.99781511 0.99983617]]\n",
      "loss:  0.29422844389201586\n",
      "---Ypred---and--Y--side--by--side-----\n",
      " \n",
      "[[0.16061856 0.        ]\n",
      " [0.00004506 0.        ]\n",
      " [0.00025372 0.        ]\n",
      " [0.89004036 1.        ]\n",
      " [0.99781511 1.        ]\n",
      " [0.99983617 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Basic simple one dimensional setting to see if some fundamentals work\n",
    "Lin1 = Linear(2,1)\n",
    "#Lin1.W = np.array([[0.6,-0.5]])\n",
    "S1 = Sigmoid()\n",
    "L = NLL_loss()\n",
    "NN = nn([Lin1, S1], L)\n",
    "test_nn(X_simple_1D, Y_simple_1D, NN, 5000, 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e897c0d",
   "metadata": {},
   "source": [
    "Linearly seprable set in two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "395070b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Ypred---- and--loss--BEFORE---training----\n",
      "\n",
      "prediction:  [[0.52373215 1.         0.99999899 0.36586441 0.39771382 0.99999995\n",
      "  0.21081829]]\n",
      "loss:  55.71104964235848\n",
      "------Ypred---- and--loss--AFTER---training----\n",
      "\n",
      "prediction  [[0.9912327  0.00004586 0.00510379 0.9930691  0.99922541 0.00042403\n",
      "  0.99999821]]\n",
      "loss:  0.0221245097686201\n",
      "---Ypred---and--Y--side--by--side-----\n",
      " \n",
      "[[0.9912327  1.        ]\n",
      " [0.00004586 0.        ]\n",
      " [0.00510379 0.        ]\n",
      " [0.9930691  1.        ]\n",
      " [0.99922541 1.        ]\n",
      " [0.00042403 0.        ]\n",
      " [0.99999821 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Linearly separable but this time two dimensional set \n",
    "np.set_printoptions(suppress=True)\n",
    "Lin1 = Linear(3,3)\n",
    "'''\n",
    "Interestingly, this network requires some hand picking while initializing weights in the the two linear layers\n",
    "Without this hand adjustment it does not perform well. I guess that the original randomization produces too small values\n",
    "'''\n",
    "Lin1.W = np.array([[0.3, -2, 1.5], [-4.2, -0.9, 1.8], [1, -0.5, 0.9]])\n",
    "S1 = ReLU()\n",
    "Lin2 = Linear(3, 1)\n",
    "Lin2.W = np.array([[1,2, -0.5]])\n",
    "S2 = Sigmoid()\n",
    "L = NLL_loss()\n",
    "NN = nn([Lin1, S1, Lin2, S2], L)\n",
    "test_nn(X_simple_2D, Y_simple_2D, NN, 1000, 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332e058",
   "metadata": {},
   "source": [
    "Testing the XOR data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "id": "361fb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Ypred---- and--loss--BEFORE---training----\n",
      "\n",
      "prediction:  [[0.24113505 0.00033206 0.99712186 0.35810752]]\n",
      "loss:  8.732330501143595\n",
      "------Ypred---- and--loss--AFTER---training----\n",
      "\n",
      "prediction  [[0.03499658 0.85405451 0.97004196 0.00444279]]\n",
      "loss:  0.22825252891766884\n",
      "---Ypred---and--Y--side--by--side-----\n",
      " \n",
      "[[0.03499658 0.        ]\n",
      " [0.85405451 1.        ]\n",
      " [0.97004196 1.        ]\n",
      " [0.00444279 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Caution: As before, default weight initialization does not seem to give good results\n",
    "One needs to tweak that initialization and lrate to get good results\n",
    "'''\n",
    "Lin1 = Linear(3,8)\n",
    "Lin1.W = np.random.normal(loc = 0., scale=2, size=(8,3))\n",
    "S1 = ReLU()\n",
    "Lin2 = Linear(8, 1)\n",
    "Lin2.W = np.random.normal(loc = 0., scale=3, size=(1,8))\n",
    "S2 = Sigmoid()\n",
    "L = NLL_loss()\n",
    "NN = nn([Lin1, S1, Lin2, S2], L)\n",
    "test_nn(X_xor, Y_xor, NN, 3000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35823c1f",
   "metadata": {},
   "source": [
    "#### Neural Network Regression with the mtcars data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "7b23f946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before training  299.8733737339507\n",
      "loss after training  84.12500605463816\n",
      "The RMSE is 4.04\n"
     ]
    }
   ],
   "source": [
    "# Network with two linear modules followed by ReLU activation and a quadratic loss module\n",
    "L1 = Linear(12, 36)\n",
    "R1 = ReLU()\n",
    "L2 = Linear(36,24)\n",
    "R2 = ReLU()\n",
    "L3 = Linear(24, 1)\n",
    "R4 = Identity()\n",
    "Q = Quadratic_loss()\n",
    "NN = nn([L1, R1, L2, R2, L3, R4], Q)\n",
    "\n",
    "\n",
    "Y_pred = NN.forward(X_train)\n",
    "\n",
    "Y_pred_original = Y_pred\n",
    "Y_pred_original_test = NN.forward(X_test)\n",
    "\n",
    "\n",
    "print('loss before training ', Q.loss(Y_pred, Y_train).sum())\n",
    "NN.train(X_train, Y_train, 15000, 0.0025)\n",
    "\n",
    "Y_pred = NN.forward(X_train)\n",
    "print('loss after training ', Q.loss(Y_pred, Y_train).sum())\n",
    "RMSE = ((Y_pred * sigma_mpg - Y_train * sigma_mpg)**2).sum()/Y_train.shape[1]\n",
    "RMSE = RMSE**0.5 \n",
    "print('The RMSE is {:.2f}'.format(RMSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "id": "17640f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_pred_original</th>\n",
       "      <th>Y_pred_trained</th>\n",
       "      <th>Y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>14.362293</td>\n",
       "      <td>14.989206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>16.308400</td>\n",
       "      <td>17.993040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>15.906414</td>\n",
       "      <td>15.990484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>14.610603</td>\n",
       "      <td>14.989206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>15.515206</td>\n",
       "      <td>13.987929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>24.003141</td>\n",
       "      <td>27.004542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>38.044107</td>\n",
       "      <td>44.026267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>32.535737</td>\n",
       "      <td>32.010932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>26.613805</td>\n",
       "      <td>28.005820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>23.445918</td>\n",
       "      <td>25.424451</td>\n",
       "      <td>31.009654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Y_pred_original  Y_pred_trained    Y_train\n",
       "0          23.445918       14.362293  14.989206\n",
       "1          23.445918       16.308400  17.993040\n",
       "2          23.445918       15.906414  15.990484\n",
       "3          23.445918       14.610603  14.989206\n",
       "4          23.445918       15.515206  13.987929\n",
       "..               ...             ...        ...\n",
       "309        23.445918       24.003141  27.004542\n",
       "310        23.445918       38.044107  44.026267\n",
       "311        23.445918       32.535737  32.010932\n",
       "312        23.445918       26.613805  28.005820\n",
       "313        23.445918       25.424451  31.009654\n",
       "\n",
       "[314 rows x 3 columns]"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(np.vstack((Y_pred_original*sigma_mpg+mean_mpg, Y_pred*sigma_mpg+mean_mpg, Y_train*sigma_mpg+mean_mpg)).T, \\\n",
    "             columns=['Y_pred_original', 'Y_pred_trained', 'Y_train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a133b",
   "metadata": {},
   "source": [
    "Now we let's see what will be the RSE on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "id": "ca0704e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the test data set is 3.79\n",
      "The RMSE on the test data before training was 8.48\n"
     ]
    }
   ],
   "source": [
    "Y_pred = NN.forward(X_test)\n",
    "RMSE = ((Y_pred*sigma_mpg-Y_test*sigma_mpg)**2).sum()/Y_test.shape[1]\n",
    "RMSE = RMSE**0.5 \n",
    "print('The RMSE on the test data set after training is {:.2f}'.format(RMSE))\n",
    "\n",
    "\n",
    "RMSE_org = ((Y_pred_original_test*sigma_mpg-Y_test*sigma_mpg)**2).sum()/Y_test.shape[1]\n",
    "RMSE_org = RMSE_org**0.5 \n",
    "print('The RMSE on the test data before training was {:.2f}'.format(RMSE_org))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
