{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f5c57b",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "The following tasks will be completed withing this notebook:\n",
    "\n",
    "1. Load from a local tsv file the mtcars data, building two sets of features with some fields one hot encoded, others standarized or kept as the are. \n",
    "\n",
    "2. Implement a function that takes a positive integer and a data set and retturns a the polynomial extended features. \n",
    "\n",
    "3. Implement the quadratic loss function with regularization and corresponding gradient functions\n",
    "\n",
    "4. Implement stochastic gradient descent function that will return a parameter fit\n",
    "\n",
    "5. Implement a cross validation procedure and search for a numner of lamda regularizations in $[0,1]$ for the best lambda and best feature set and polynomial order features\n",
    "\n",
    "6. Come up with a direct algebaic solution and compare the RMSE of this one with the best obtained in 5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da03e32",
   "metadata": {},
   "source": [
    "### Load data and make feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b7252f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "    takes path and returns a pnadas data frame object with the data from file under path\n",
    "    '''\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for row in csv.DictReader(f, delimiter='\\t'):\n",
    "            data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def one_hot(x):\n",
    "    '''\n",
    "    @param x : pandas series object\n",
    "    returns a data frame with the original series and the one hot encoded fields\n",
    "    '''\n",
    "    \n",
    "    new_frame = pd.DataFrame(x)\n",
    "    for val in x.unique():\n",
    "        _name = str(int(val))\n",
    "        new_frame[x.name + _name] = new_frame.apply(lambda col: 1 if col[x.name] == val else 0, axis=1)\n",
    "    return new_frame.iloc[:,1:]\n",
    "        \n",
    "    \n",
    "def standardize(x):\n",
    "    '''\n",
    "    @param x : string holding the name of the field to be standarized\n",
    "    '''\n",
    "    z = x.astype('float')\n",
    "    result = np.array((z - np.mean(z))/np.std(z))\n",
    "\n",
    "    return pd.DataFrame(result, columns=[x.name])\n",
    "\n",
    "\n",
    "def make_features(feature_list):\n",
    "    '''\n",
    "    @param feature_list: a list of tuples, first entry is a pandas series and the next one is a string with \n",
    "    onehot or standardize. \n",
    "    '''\n",
    "    new_features = []\n",
    "    for f,ftype in feature_list:\n",
    "        if ftype == 'onehot':\n",
    "            new_features.append(one_hot(f))\n",
    "        elif ftype == 'standardize':\n",
    "            new_features.append(standardize(f))\n",
    "        else:\n",
    "            new_features.append(f)\n",
    "        \n",
    "    return pd.concat(new_features, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "auto_data = load_data('../code_and_data_for_hw05/auto-mpg-regression.tsv')\n",
    "\n",
    "features1 = [\n",
    "            (auto_data.cylinders, 'standardize'),\n",
    "            (auto_data.displacement, 'standardize'),\n",
    "            (auto_data.horsepower,'standardize'),\n",
    "            (auto_data.weight, 'standardize'),\n",
    "            (auto_data.acceleration, 'standardize'),\n",
    "            (auto_data.origin, 'onehot'),\n",
    "            (auto_data.mpg, 'standardize')\n",
    "            ]\n",
    "\n",
    "features2 = [\n",
    "            (auto_data.cylinders, 'onehot'),\n",
    "            (auto_data.displacement, 'standardize'),\n",
    "            (auto_data.horsepower, 'standardize'),\n",
    "            (auto_data.weight, 'standardize'),\n",
    "            (auto_data.acceleration, 'standardize'),\n",
    "            (auto_data.origin, 'onehot'),\n",
    "            (auto_data.mpg, 'standardize')\n",
    "            ]\n",
    "\n",
    "\n",
    "auto_data_1 = make_features(features1)\n",
    "auto_data_2 = make_features(features2)\n",
    "\n",
    "# Keep this for future reference:\n",
    "mean_mpg, sigma_mpg = auto_data['mpg'].astype('float').mean(), auto_data['mpg'].astype('float').std() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ecdf3",
   "metadata": {},
   "source": [
    "### Make polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d3dd5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_polynomial_features(data, k):\n",
    "    '''\n",
    "    @param data: takes and array or data frame with features as columns\n",
    "    @param k: int greater than 1\n",
    "    returns an numpy array with polynomialy transformed features up to poly order k\n",
    "    '''\n",
    "    assert type(k)==int and k>=1\n",
    "    \n",
    "    raw_features = np.array(data)\n",
    "    n,d = raw_features.shape\n",
    "    if k == 1:\n",
    "        return raw_features\n",
    "    else:\n",
    "        new_features = []\n",
    "        # choose all k tuples from feature columns\n",
    "        for i in range(1, k+1, 1):\n",
    "            for t in itertools.combinations_with_replacement(range(d), i):\n",
    "                tmp = np.ones((n,1))\n",
    "                for j in range(len(t)): \n",
    "                    tmp = tmp*raw_features[:, [t[j]]]\n",
    "\n",
    "                new_features.append(tmp)\n",
    "\n",
    "    return np.hstack(new_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274c4ee",
   "metadata": {},
   "source": [
    "### Quadratic loss function and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2b74d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def quadratic_loss(data, labels, theta, lam):\n",
    "    '''\n",
    "    return the quadratic loss with lambda regularization\n",
    "    '''\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    theta = np.array(theta)\n",
    "    \n",
    "    n,d = data.shape\n",
    "    new_data = np.hstack((data, np.ones((n,1))))\n",
    "    \n",
    "    return (1/n) * np.linalg.norm((np.matmul(new_data, theta) - labels))**2 + lam * np.linalg.norm(theta[:,:-1])**2 \n",
    "\n",
    "\n",
    "def quadratic_loss_grad(data, labels, theta, lam):\n",
    "    '''\n",
    "    return the quadratic loss with lambda regularization gradient\n",
    "    '''\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    theta = np.array(theta)\n",
    "    \n",
    "    n,d = data.shape\n",
    "    new_data = np.hstack((data, np.ones((n,1))))\n",
    "\n",
    "    \n",
    "    d_th = (1/n) * np.matmul(2*(np.matmul(new_data, theta) - labels).T, data).T + 2 * lam * theta[:-1,:]\n",
    "    d_th0 = (1/n) * np.matmul(2*(np.matmul(new_data, theta) - labels).T, np.ones((n,1)))\n",
    "\n",
    "    return np.vstack((d_th, d_th0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fef489",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "dfe9e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(loss, loss_grad, w0, data, labels, lam, step_size_fn, max_iter=1000, stochastic = True):\n",
    "    '''\n",
    "    Performs stochastic (or normal if stochastic=False) gradient descent\n",
    "    \n",
    "    return the fitted set of weigths w0, a list of intermiadaty loss values in a list loss_list and \n",
    "    a list of intermediary weights ws\n",
    "    '''\n",
    "    counter = 0\n",
    "    n,d = data.shape\n",
    "    \n",
    "    loss_list = [loss(data, labels, w0, lam)]\n",
    "    ws = [w0]\n",
    "    \n",
    "    while counter <= max_iter:\n",
    "        \n",
    "        counter += 1\n",
    "        if stochastic:\n",
    "            j = np.random.choice(n)\n",
    "            delta = loss_grad(data[[j],:], labels[[j],0], w0, lam)\n",
    "        else:\n",
    "            delta = loss_grad(data, labels, w0, lam)\n",
    "\n",
    "                \n",
    "        w1 = w0 - step_size_fn(counter) * delta\n",
    "        w0 = w1\n",
    "    \n",
    "        ws.append(w0)\n",
    "        if stochastic:\n",
    "            loss_list.append(loss(data[[j],:], labels[[j],0], w0, lam))\n",
    "        else:\n",
    "            loss_list.append(loss(data, labels, w0, lam))\n",
    "        \n",
    "    \n",
    "    return (w0, loss_list, ws)\n",
    "\n",
    "\n",
    "def step_size_fn(i):\n",
    "    return 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd9fc0",
   "metadata": {},
   "source": [
    "Run the SGD for one set of features and with an arbitratry chosen lambda = 0.1 just to see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cb95460a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss went from 22.00338293967154 to 0.0019257274144147585 in 1000 iterations\n"
     ]
    }
   ],
   "source": [
    "data = np.array(auto_data_1.iloc[:,:-1])\n",
    "theta = np.ones((data.shape[1]+1,1))\n",
    "lam = 0.1\n",
    "labels = np.array(auto_data_1.iloc[:,[-1]])\n",
    "\n",
    "a,b,c = sgd(quadratic_loss, quadratic_loss_grad, theta, data, labels, lam, step_size_fn)\n",
    "\n",
    "plt.plot(range(len(b)), b)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('quadratic loss')\n",
    "plt.show()\n",
    "print('The loss went from {} to {} in 1000 iterations'.format(b[0], b[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7cc9fd",
   "metadata": {},
   "source": [
    "### Cross validation and searching for optimal hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2503e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(prediction, labels):\n",
    "    '''\n",
    "    @params: prediction, labels column vectors of same size (nx1)\n",
    "    \n",
    "    returns the RMS based on prediction and labels\n",
    "    '''\n",
    "    n = labels.shape[0]\n",
    "    assert labels.shape == prediction.shape and labels.shape[1] == 1\n",
    "    return ((1/n) * float(np.sum(np.matmul(prediction-labels, (prediction-labels).T))))**0.5\n",
    "    \n",
    "\n",
    "def eval_learner(\n",
    "        loss, \n",
    "        loss_grad, \n",
    "        data_train, \n",
    "        labels_train, \n",
    "        data_test, \n",
    "        labels_test, \n",
    "        step_size_fn, \n",
    "        my_lam \n",
    "                ):\n",
    "    \n",
    "    '''\n",
    "    returns the quadratic loss calculated on the test data set after fitting with SGD on the training data set\n",
    "    '''\n",
    "    n,d = data_train.shape\n",
    "\n",
    "    theta = sgd(loss, loss_grad, np.zeros((d+1,1)), data_train, labels_train, my_lam, step_size_fn)[0]\n",
    "    return quadratic_loss(data_test, labels_test, theta, my_lam)\n",
    "\n",
    "    \n",
    "def cross_validate(loss, loss_grad, data, labels, my_lam, step_size_fn,  k):\n",
    "    '''\n",
    "    perform k-fold cross validation and returns the averaged loss\n",
    "    '''\n",
    "    data_labels = np.hstack((data, labels))\n",
    "    np.random.shuffle(data_labels) \n",
    "    \n",
    "    results = 0\n",
    "    \n",
    "    data_split = np.array_split(data_labels, k, axis=0)\n",
    "    \n",
    "    for i in range(k):\n",
    "        one_out = np.concatenate(data_split[0:i]+data_split[i+1:],axis=0)\n",
    "        one = data_split[i]\n",
    "    \n",
    "        data_train,labels_train = one_out[:,:-1], one_out[:,[-1]]\n",
    "\n",
    "        data_test,labels_test = one[:,:-1], one[:,[-1]]\n",
    "        \n",
    "        efficiency = eval_learner(\n",
    "                            loss, \n",
    "                            loss_grad, \n",
    "                            data_train, \n",
    "                            labels_train, \n",
    "                            data_test, \n",
    "                            labels_test, \n",
    "                            step_size_fn,\n",
    "                            my_lam\n",
    "                                 )\n",
    "        \n",
    "        results += efficiency\n",
    "    return(results/k)\n",
    "\n",
    "#cross_validate(quadratic_loss, quadratic_loss_grad, data, labels, 0.1, step_size_fn, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353eb961",
   "metadata": {},
   "source": [
    "Now run cross validation for features1 and features2 and for regular features and polynomial of order 1, 2 & 3 features and for lambdas ranging from 0.0, 0.02, ...., 1\n",
    "\n",
    "Finally we retrieve the optimal set of hyper paramters and fit theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0f9f422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on feature set 1, poly order 1,  lambda 0.0\n",
      "Working on feature set 1, poly order 1,  lambda 0.02\n",
      "Working on feature set 1, poly order 1,  lambda 0.04\n",
      "Working on feature set 1, poly order 1,  lambda 0.06\n",
      "Working on feature set 1, poly order 1,  lambda 0.08\n",
      "Working on feature set 1, poly order 1,  lambda 0.1\n",
      "Working on feature set 1, poly order 1,  lambda 0.12\n",
      "Working on feature set 1, poly order 1,  lambda 0.14\n",
      "Working on feature set 1, poly order 1,  lambda 0.16\n",
      "Working on feature set 1, poly order 1,  lambda 0.18\n",
      "Working on feature set 1, poly order 1,  lambda 0.2\n",
      "Working on feature set 1, poly order 1,  lambda 0.22\n",
      "Working on feature set 1, poly order 1,  lambda 0.24\n",
      "Working on feature set 1, poly order 1,  lambda 0.26\n",
      "Working on feature set 1, poly order 1,  lambda 0.28\n",
      "Working on feature set 1, poly order 1,  lambda 0.3\n",
      "Working on feature set 1, poly order 1,  lambda 0.32\n",
      "Working on feature set 1, poly order 1,  lambda 0.34\n",
      "Working on feature set 1, poly order 1,  lambda 0.36\n",
      "Working on feature set 1, poly order 1,  lambda 0.38\n",
      "Working on feature set 1, poly order 1,  lambda 0.4\n",
      "Working on feature set 1, poly order 1,  lambda 0.42\n",
      "Working on feature set 1, poly order 1,  lambda 0.44\n",
      "Working on feature set 1, poly order 1,  lambda 0.46\n",
      "Working on feature set 1, poly order 1,  lambda 0.48\n",
      "Working on feature set 1, poly order 1,  lambda 0.5\n",
      "Working on feature set 1, poly order 1,  lambda 0.52\n",
      "Working on feature set 1, poly order 1,  lambda 0.54\n",
      "Working on feature set 1, poly order 1,  lambda 0.56\n",
      "Working on feature set 1, poly order 1,  lambda 0.58\n",
      "Working on feature set 1, poly order 1,  lambda 0.6\n",
      "Working on feature set 1, poly order 1,  lambda 0.62\n",
      "Working on feature set 1, poly order 1,  lambda 0.64\n",
      "Working on feature set 1, poly order 1,  lambda 0.66\n",
      "Working on feature set 1, poly order 1,  lambda 0.68\n",
      "Working on feature set 1, poly order 1,  lambda 0.7000000000000001\n",
      "Working on feature set 1, poly order 1,  lambda 0.72\n",
      "Working on feature set 1, poly order 1,  lambda 0.74\n",
      "Working on feature set 1, poly order 1,  lambda 0.76\n",
      "Working on feature set 1, poly order 1,  lambda 0.78\n",
      "Working on feature set 1, poly order 1,  lambda 0.8\n",
      "Working on feature set 1, poly order 1,  lambda 0.8200000000000001\n",
      "Working on feature set 1, poly order 1,  lambda 0.84\n",
      "Working on feature set 1, poly order 1,  lambda 0.86\n",
      "Working on feature set 1, poly order 1,  lambda 0.88\n",
      "Working on feature set 1, poly order 1,  lambda 0.9\n",
      "Working on feature set 1, poly order 1,  lambda 0.92\n",
      "Working on feature set 1, poly order 1,  lambda 0.9400000000000001\n",
      "Working on feature set 1, poly order 1,  lambda 0.96\n",
      "Working on feature set 1, poly order 1,  lambda 0.98\n",
      "Working on feature set 1, poly order 1,  lambda 1.0\n",
      "Working on feature set 1, poly order 2,  lambda 0.0\n",
      "Working on feature set 1, poly order 2,  lambda 0.02\n",
      "Working on feature set 1, poly order 2,  lambda 0.04\n",
      "Working on feature set 1, poly order 2,  lambda 0.06\n",
      "Working on feature set 1, poly order 2,  lambda 0.08\n",
      "Working on feature set 1, poly order 2,  lambda 0.1\n",
      "Working on feature set 1, poly order 2,  lambda 0.12\n",
      "Working on feature set 1, poly order 2,  lambda 0.14\n",
      "Working on feature set 1, poly order 2,  lambda 0.16\n",
      "Working on feature set 1, poly order 2,  lambda 0.18\n",
      "Working on feature set 1, poly order 2,  lambda 0.2\n",
      "Working on feature set 1, poly order 2,  lambda 0.22\n",
      "Working on feature set 1, poly order 2,  lambda 0.24\n",
      "Working on feature set 1, poly order 2,  lambda 0.26\n",
      "Working on feature set 1, poly order 2,  lambda 0.28\n",
      "Working on feature set 1, poly order 2,  lambda 0.3\n",
      "Working on feature set 1, poly order 2,  lambda 0.32\n",
      "Working on feature set 1, poly order 2,  lambda 0.34\n",
      "Working on feature set 1, poly order 2,  lambda 0.36\n",
      "Working on feature set 1, poly order 2,  lambda 0.38\n",
      "Working on feature set 1, poly order 2,  lambda 0.4\n",
      "Working on feature set 1, poly order 2,  lambda 0.42\n",
      "Working on feature set 1, poly order 2,  lambda 0.44\n",
      "Working on feature set 1, poly order 2,  lambda 0.46\n",
      "Working on feature set 1, poly order 2,  lambda 0.48\n",
      "Working on feature set 1, poly order 2,  lambda 0.5\n",
      "Working on feature set 1, poly order 2,  lambda 0.52\n",
      "Working on feature set 1, poly order 2,  lambda 0.54\n",
      "Working on feature set 1, poly order 2,  lambda 0.56\n",
      "Working on feature set 1, poly order 2,  lambda 0.58\n",
      "Working on feature set 1, poly order 2,  lambda 0.6\n",
      "Working on feature set 1, poly order 2,  lambda 0.62\n",
      "Working on feature set 1, poly order 2,  lambda 0.64\n",
      "Working on feature set 1, poly order 2,  lambda 0.66\n",
      "Working on feature set 1, poly order 2,  lambda 0.68\n",
      "Working on feature set 1, poly order 2,  lambda 0.7000000000000001\n",
      "Working on feature set 1, poly order 2,  lambda 0.72\n",
      "Working on feature set 1, poly order 2,  lambda 0.74\n",
      "Working on feature set 1, poly order 2,  lambda 0.76\n",
      "Working on feature set 1, poly order 2,  lambda 0.78\n",
      "Working on feature set 1, poly order 2,  lambda 0.8\n",
      "Working on feature set 1, poly order 2,  lambda 0.8200000000000001\n",
      "Working on feature set 1, poly order 2,  lambda 0.84\n",
      "Working on feature set 1, poly order 2,  lambda 0.86\n",
      "Working on feature set 1, poly order 2,  lambda 0.88\n",
      "Working on feature set 1, poly order 2,  lambda 0.9\n",
      "Working on feature set 1, poly order 2,  lambda 0.92\n",
      "Working on feature set 1, poly order 2,  lambda 0.9400000000000001\n",
      "Working on feature set 1, poly order 2,  lambda 0.96\n",
      "Working on feature set 1, poly order 2,  lambda 0.98\n",
      "Working on feature set 1, poly order 2,  lambda 1.0\n",
      "Working on feature set 1, poly order 3,  lambda 0.0\n",
      "Working on feature set 1, poly order 3,  lambda 0.02\n",
      "Working on feature set 1, poly order 3,  lambda 0.04\n",
      "Working on feature set 1, poly order 3,  lambda 0.06\n",
      "Working on feature set 1, poly order 3,  lambda 0.08\n",
      "Working on feature set 1, poly order 3,  lambda 0.1\n",
      "Working on feature set 1, poly order 3,  lambda 0.12\n",
      "Working on feature set 1, poly order 3,  lambda 0.14\n",
      "Working on feature set 1, poly order 3,  lambda 0.16\n",
      "Working on feature set 1, poly order 3,  lambda 0.18\n",
      "Working on feature set 1, poly order 3,  lambda 0.2\n",
      "Working on feature set 1, poly order 3,  lambda 0.22\n",
      "Working on feature set 1, poly order 3,  lambda 0.24\n",
      "Working on feature set 1, poly order 3,  lambda 0.26\n",
      "Working on feature set 1, poly order 3,  lambda 0.28\n",
      "Working on feature set 1, poly order 3,  lambda 0.3\n",
      "Working on feature set 1, poly order 3,  lambda 0.32\n",
      "Working on feature set 1, poly order 3,  lambda 0.34\n",
      "Working on feature set 1, poly order 3,  lambda 0.36\n",
      "Working on feature set 1, poly order 3,  lambda 0.38\n",
      "Working on feature set 1, poly order 3,  lambda 0.4\n",
      "Working on feature set 1, poly order 3,  lambda 0.42\n",
      "Working on feature set 1, poly order 3,  lambda 0.44\n",
      "Working on feature set 1, poly order 3,  lambda 0.46\n",
      "Working on feature set 1, poly order 3,  lambda 0.48\n",
      "Working on feature set 1, poly order 3,  lambda 0.5\n",
      "Working on feature set 1, poly order 3,  lambda 0.52\n",
      "Working on feature set 1, poly order 3,  lambda 0.54\n",
      "Working on feature set 1, poly order 3,  lambda 0.56\n",
      "Working on feature set 1, poly order 3,  lambda 0.58\n",
      "Working on feature set 1, poly order 3,  lambda 0.6\n",
      "Working on feature set 1, poly order 3,  lambda 0.62\n",
      "Working on feature set 1, poly order 3,  lambda 0.64\n",
      "Working on feature set 1, poly order 3,  lambda 0.66\n",
      "Working on feature set 1, poly order 3,  lambda 0.68\n",
      "Working on feature set 1, poly order 3,  lambda 0.7000000000000001\n",
      "Working on feature set 1, poly order 3,  lambda 0.72\n",
      "Working on feature set 1, poly order 3,  lambda 0.74\n",
      "Working on feature set 1, poly order 3,  lambda 0.76\n",
      "Working on feature set 1, poly order 3,  lambda 0.78\n",
      "Working on feature set 1, poly order 3,  lambda 0.8\n",
      "Working on feature set 1, poly order 3,  lambda 0.8200000000000001\n",
      "Working on feature set 1, poly order 3,  lambda 0.84\n",
      "Working on feature set 1, poly order 3,  lambda 0.86\n",
      "Working on feature set 1, poly order 3,  lambda 0.88\n",
      "Working on feature set 1, poly order 3,  lambda 0.9\n",
      "Working on feature set 1, poly order 3,  lambda 0.92\n",
      "Working on feature set 1, poly order 3,  lambda 0.9400000000000001\n",
      "Working on feature set 1, poly order 3,  lambda 0.96\n",
      "Working on feature set 1, poly order 3,  lambda 0.98\n",
      "Working on feature set 1, poly order 3,  lambda 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on feature set 2, poly order 1,  lambda 0.0\n",
      "Working on feature set 2, poly order 1,  lambda 0.02\n",
      "Working on feature set 2, poly order 1,  lambda 0.04\n",
      "Working on feature set 2, poly order 1,  lambda 0.06\n",
      "Working on feature set 2, poly order 1,  lambda 0.08\n",
      "Working on feature set 2, poly order 1,  lambda 0.1\n",
      "Working on feature set 2, poly order 1,  lambda 0.12\n",
      "Working on feature set 2, poly order 1,  lambda 0.14\n",
      "Working on feature set 2, poly order 1,  lambda 0.16\n",
      "Working on feature set 2, poly order 1,  lambda 0.18\n",
      "Working on feature set 2, poly order 1,  lambda 0.2\n",
      "Working on feature set 2, poly order 1,  lambda 0.22\n",
      "Working on feature set 2, poly order 1,  lambda 0.24\n",
      "Working on feature set 2, poly order 1,  lambda 0.26\n",
      "Working on feature set 2, poly order 1,  lambda 0.28\n",
      "Working on feature set 2, poly order 1,  lambda 0.3\n",
      "Working on feature set 2, poly order 1,  lambda 0.32\n",
      "Working on feature set 2, poly order 1,  lambda 0.34\n",
      "Working on feature set 2, poly order 1,  lambda 0.36\n",
      "Working on feature set 2, poly order 1,  lambda 0.38\n",
      "Working on feature set 2, poly order 1,  lambda 0.4\n",
      "Working on feature set 2, poly order 1,  lambda 0.42\n",
      "Working on feature set 2, poly order 1,  lambda 0.44\n",
      "Working on feature set 2, poly order 1,  lambda 0.46\n",
      "Working on feature set 2, poly order 1,  lambda 0.48\n",
      "Working on feature set 2, poly order 1,  lambda 0.5\n",
      "Working on feature set 2, poly order 1,  lambda 0.52\n",
      "Working on feature set 2, poly order 1,  lambda 0.54\n",
      "Working on feature set 2, poly order 1,  lambda 0.56\n",
      "Working on feature set 2, poly order 1,  lambda 0.58\n",
      "Working on feature set 2, poly order 1,  lambda 0.6\n",
      "Working on feature set 2, poly order 1,  lambda 0.62\n",
      "Working on feature set 2, poly order 1,  lambda 0.64\n",
      "Working on feature set 2, poly order 1,  lambda 0.66\n",
      "Working on feature set 2, poly order 1,  lambda 0.68\n",
      "Working on feature set 2, poly order 1,  lambda 0.7000000000000001\n",
      "Working on feature set 2, poly order 1,  lambda 0.72\n",
      "Working on feature set 2, poly order 1,  lambda 0.74\n",
      "Working on feature set 2, poly order 1,  lambda 0.76\n",
      "Working on feature set 2, poly order 1,  lambda 0.78\n",
      "Working on feature set 2, poly order 1,  lambda 0.8\n",
      "Working on feature set 2, poly order 1,  lambda 0.8200000000000001\n",
      "Working on feature set 2, poly order 1,  lambda 0.84\n",
      "Working on feature set 2, poly order 1,  lambda 0.86\n",
      "Working on feature set 2, poly order 1,  lambda 0.88\n",
      "Working on feature set 2, poly order 1,  lambda 0.9\n",
      "Working on feature set 2, poly order 1,  lambda 0.92\n",
      "Working on feature set 2, poly order 1,  lambda 0.9400000000000001\n",
      "Working on feature set 2, poly order 1,  lambda 0.96\n",
      "Working on feature set 2, poly order 1,  lambda 0.98\n",
      "Working on feature set 2, poly order 1,  lambda 1.0\n",
      "Working on feature set 2, poly order 2,  lambda 0.0\n",
      "Working on feature set 2, poly order 2,  lambda 0.02\n",
      "Working on feature set 2, poly order 2,  lambda 0.04\n",
      "Working on feature set 2, poly order 2,  lambda 0.06\n",
      "Working on feature set 2, poly order 2,  lambda 0.08\n",
      "Working on feature set 2, poly order 2,  lambda 0.1\n",
      "Working on feature set 2, poly order 2,  lambda 0.12\n",
      "Working on feature set 2, poly order 2,  lambda 0.14\n",
      "Working on feature set 2, poly order 2,  lambda 0.16\n",
      "Working on feature set 2, poly order 2,  lambda 0.18\n",
      "Working on feature set 2, poly order 2,  lambda 0.2\n",
      "Working on feature set 2, poly order 2,  lambda 0.22\n",
      "Working on feature set 2, poly order 2,  lambda 0.24\n",
      "Working on feature set 2, poly order 2,  lambda 0.26\n",
      "Working on feature set 2, poly order 2,  lambda 0.28\n",
      "Working on feature set 2, poly order 2,  lambda 0.3\n",
      "Working on feature set 2, poly order 2,  lambda 0.32\n",
      "Working on feature set 2, poly order 2,  lambda 0.34\n",
      "Working on feature set 2, poly order 2,  lambda 0.36\n",
      "Working on feature set 2, poly order 2,  lambda 0.38\n",
      "Working on feature set 2, poly order 2,  lambda 0.4\n",
      "Working on feature set 2, poly order 2,  lambda 0.42\n",
      "Working on feature set 2, poly order 2,  lambda 0.44\n",
      "Working on feature set 2, poly order 2,  lambda 0.46\n",
      "Working on feature set 2, poly order 2,  lambda 0.48\n",
      "Working on feature set 2, poly order 2,  lambda 0.5\n",
      "Working on feature set 2, poly order 2,  lambda 0.52\n",
      "Working on feature set 2, poly order 2,  lambda 0.54\n",
      "Working on feature set 2, poly order 2,  lambda 0.56\n",
      "Working on feature set 2, poly order 2,  lambda 0.58\n",
      "Working on feature set 2, poly order 2,  lambda 0.6\n",
      "Working on feature set 2, poly order 2,  lambda 0.62\n",
      "Working on feature set 2, poly order 2,  lambda 0.64\n",
      "Working on feature set 2, poly order 2,  lambda 0.66\n",
      "Working on feature set 2, poly order 2,  lambda 0.68\n",
      "Working on feature set 2, poly order 2,  lambda 0.7000000000000001\n",
      "Working on feature set 2, poly order 2,  lambda 0.72\n",
      "Working on feature set 2, poly order 2,  lambda 0.74\n",
      "Working on feature set 2, poly order 2,  lambda 0.76\n",
      "Working on feature set 2, poly order 2,  lambda 0.78\n",
      "Working on feature set 2, poly order 2,  lambda 0.8\n",
      "Working on feature set 2, poly order 2,  lambda 0.8200000000000001\n",
      "Working on feature set 2, poly order 2,  lambda 0.84\n",
      "Working on feature set 2, poly order 2,  lambda 0.86\n",
      "Working on feature set 2, poly order 2,  lambda 0.88\n",
      "Working on feature set 2, poly order 2,  lambda 0.9\n",
      "Working on feature set 2, poly order 2,  lambda 0.92\n",
      "Working on feature set 2, poly order 2,  lambda 0.9400000000000001\n",
      "Working on feature set 2, poly order 2,  lambda 0.96\n",
      "Working on feature set 2, poly order 2,  lambda 0.98\n",
      "Working on feature set 2, poly order 2,  lambda 1.0\n",
      "Working on feature set 2, poly order 3,  lambda 0.0\n",
      "Working on feature set 2, poly order 3,  lambda 0.02\n",
      "Working on feature set 2, poly order 3,  lambda 0.04\n",
      "Working on feature set 2, poly order 3,  lambda 0.06\n",
      "Working on feature set 2, poly order 3,  lambda 0.08\n",
      "Working on feature set 2, poly order 3,  lambda 0.1\n",
      "Working on feature set 2, poly order 3,  lambda 0.12\n",
      "Working on feature set 2, poly order 3,  lambda 0.14\n",
      "Working on feature set 2, poly order 3,  lambda 0.16\n",
      "Working on feature set 2, poly order 3,  lambda 0.18\n",
      "Working on feature set 2, poly order 3,  lambda 0.2\n",
      "Working on feature set 2, poly order 3,  lambda 0.22\n",
      "Working on feature set 2, poly order 3,  lambda 0.24\n",
      "Working on feature set 2, poly order 3,  lambda 0.26\n",
      "Working on feature set 2, poly order 3,  lambda 0.28\n",
      "Working on feature set 2, poly order 3,  lambda 0.3\n",
      "Working on feature set 2, poly order 3,  lambda 0.32\n",
      "Working on feature set 2, poly order 3,  lambda 0.34\n",
      "Working on feature set 2, poly order 3,  lambda 0.36\n",
      "Working on feature set 2, poly order 3,  lambda 0.38\n",
      "Working on feature set 2, poly order 3,  lambda 0.4\n",
      "Working on feature set 2, poly order 3,  lambda 0.42\n",
      "Working on feature set 2, poly order 3,  lambda 0.44\n",
      "Working on feature set 2, poly order 3,  lambda 0.46\n",
      "Working on feature set 2, poly order 3,  lambda 0.48\n",
      "Working on feature set 2, poly order 3,  lambda 0.5\n",
      "Working on feature set 2, poly order 3,  lambda 0.52\n",
      "Working on feature set 2, poly order 3,  lambda 0.54\n",
      "Working on feature set 2, poly order 3,  lambda 0.56\n",
      "Working on feature set 2, poly order 3,  lambda 0.58\n",
      "Working on feature set 2, poly order 3,  lambda 0.6\n",
      "Working on feature set 2, poly order 3,  lambda 0.62\n",
      "Working on feature set 2, poly order 3,  lambda 0.64\n",
      "Working on feature set 2, poly order 3,  lambda 0.66\n",
      "Working on feature set 2, poly order 3,  lambda 0.68\n",
      "Working on feature set 2, poly order 3,  lambda 0.7000000000000001\n",
      "Working on feature set 2, poly order 3,  lambda 0.72\n",
      "Working on feature set 2, poly order 3,  lambda 0.74\n",
      "Working on feature set 2, poly order 3,  lambda 0.76\n",
      "Working on feature set 2, poly order 3,  lambda 0.78\n",
      "Working on feature set 2, poly order 3,  lambda 0.8\n",
      "Working on feature set 2, poly order 3,  lambda 0.8200000000000001\n",
      "Working on feature set 2, poly order 3,  lambda 0.84\n",
      "Working on feature set 2, poly order 3,  lambda 0.86\n",
      "Working on feature set 2, poly order 3,  lambda 0.88\n",
      "Working on feature set 2, poly order 3,  lambda 0.9\n",
      "Working on feature set 2, poly order 3,  lambda 0.92\n",
      "Working on feature set 2, poly order 3,  lambda 0.9400000000000001\n",
      "Working on feature set 2, poly order 3,  lambda 0.96\n",
      "Working on feature set 2, poly order 3,  lambda 0.98\n",
      "Working on feature set 2, poly order 3,  lambda 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cross_val': 0.2625486549616546, 'feature_set': 2, 'poly_order': 1, 'lam': 0.02}\n"
     ]
    }
   ],
   "source": [
    "best = {'cross_val': 99999, 'feature_set': -9, 'poly_order': -9, 'lam': -9}\n",
    "\n",
    "\n",
    "for data_set in zip([auto_data_1, auto_data_2],range(1,3,1)):\n",
    "    for k in [1,2,3]:\n",
    "        for lam in [0.02*i for i in range(0,51,1)]:\n",
    "            print('Working on feature set {}, poly order {},  lambda {}'.format(data_set[1], k, lam))\n",
    "            data =  make_polynomial_features(np.array(data_set[0].iloc[:,:-1]), k)\n",
    "            labels = np.array(data_set[0].iloc[:,[-1]])\n",
    "            new  = cross_validate(quadratic_loss, quadratic_loss_grad, data, labels, lam, step_size_fn, 10)\n",
    "            if new < best['cross_val']:\n",
    "                best['cross_val'] = new\n",
    "                best['poly_order'] = k\n",
    "                best['lam'] = lam\n",
    "                best['feature_set'] = data_set[1]\n",
    "\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32ff47",
   "metadata": {},
   "source": [
    "### Comapare the SGD results with the direct solution through inverting matirces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57876e",
   "metadata": {},
   "source": [
    "First let's calculate the RMS for the best set of hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "aa86495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the best hyper parameters SGD gives a RMSE of 1.776 and a quadratic loss of 0.277\n"
     ]
    }
   ],
   "source": [
    "if best['feature_set']==1:\n",
    "    data_set = auto_data_1\n",
    "else:\n",
    "    data_set = auto_data_2\n",
    "    \n",
    "    \n",
    "data = make_polynomial_features(np.array(data_set.iloc[:,:-1]),best['poly_order'])\n",
    "labels = np.array(data_set.iloc[:,[-1]])\n",
    "lam = best['lam']\n",
    "n,d = data.shape\n",
    "\n",
    "theta_fit = sgd(quadratic_loss, quadratic_loss_grad, np.zeros((d+1,1)), data, labels, lam, step_size_fn)[0]\n",
    "\n",
    "prediction_fit = np.matmul(np.hstack((data, np.ones((n,1)))),theta_fit)\n",
    "\n",
    "print('For the best hyper parameters SGD gives a RMSE of {:.3f} and a quadratic loss of {:.3f}'\\\n",
    "      .format(RMSE(prediction_fit, labels), quadratic_loss(data, labels, theta_fit, lam)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb1a27",
   "metadata": {},
   "source": [
    "Now lets do the direct algebraic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "f5e202da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the best hyper parameters direct solution gives a RMSE of 0.016 and a quadratic loss of 0.250\n"
     ]
    }
   ],
   "source": [
    "new_data = np.hstack((data, np.ones((n,1))))\n",
    "theta_direct = np.matmul(np.matmul(np.linalg.inv(np.matmul(new_data.T, new_data) +\\\n",
    "                                                 n * lam * np.identity(d+1)), new_data.T), labels)\n",
    "\n",
    "\n",
    "prediction_direct = np.matmul(np.hstack((data, np.ones((n,1)))),theta_direct)\n",
    "\n",
    "\n",
    "print('For the best hyper parameters direct solution gives a RMSE of {:.3f} and a quadratic loss of {:.3f}'.\\\n",
    "      format(RMSE(prediction_direct, labels), quadratic_loss(data, labels, theta_direct, lam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fce1ec",
   "metadata": {},
   "source": [
    "### And just let's look at the proediction produced by SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7eef9087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of SGD is 4.101\n",
      "RMSE of algebraic method is 3.894\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>prediction</th>\n",
       "      <th>error</th>\n",
       "      <th>prediction_direct</th>\n",
       "      <th>direct_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.283039</td>\n",
       "      <td>0.015724</td>\n",
       "      <td>19.107108</td>\n",
       "      <td>0.061506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>15.135204</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>16.049385</td>\n",
       "      <td>0.069959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>16.921343</td>\n",
       "      <td>0.059925</td>\n",
       "      <td>18.003119</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>17.146691</td>\n",
       "      <td>0.071668</td>\n",
       "      <td>17.938792</td>\n",
       "      <td>0.121174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>17.551812</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>18.664808</td>\n",
       "      <td>0.097930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  prediction     error  prediction_direct  direct_error\n",
       "0  18.0   18.283039  0.015724          19.107108      0.061506\n",
       "1  15.0   15.135204  0.009014          16.049385      0.069959\n",
       "2  18.0   16.921343  0.059925          18.003119      0.000173\n",
       "3  16.0   17.146691  0.071668          17.938792      0.121174\n",
       "4  17.0   17.551812  0.032460          18.664808      0.097930"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prediction_fit_original = sigma_mpg * prediction_fit + mean_mpg\n",
    "labels_original = np.array(auto_data.mpg.astype('float')).reshape((n,1))\n",
    "\n",
    "prediction_direct_original = sigma_mpg * np.matmul(np.hstack((data, np.ones((n,1)))),theta_direct) + mean_mpg\n",
    "\n",
    "results = pd.DataFrame(\n",
    "            np.hstack((\n",
    "                    labels_original,\n",
    "                    prediction_fit_original, \n",
    "                    abs(prediction_fit_original-labels_original)/labels_original, \n",
    "                    prediction_direct_original, \n",
    "                    abs(prediction_direct_original-labels_original)/labels_original \n",
    "                    ))\n",
    "            , columns=['mpg', 'prediction', 'error', 'prediction_direct', 'direct_error'])\n",
    "                       \n",
    "\n",
    "SSE = ((results['prediction'] - results['mpg'])**2).sum()\n",
    "print ('RMSE of SGD is {:.3f}'.format((SSE/n)**0.5))\n",
    "\n",
    "SSE_direct = ((results['prediction_direct'] - results['mpg'])**2).sum()\n",
    "print ('RMSE of algebraic method is {:.3f}'.format((SSE_direct/n)**0.5))\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca9407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/site-packages (2.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
