{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6c0295",
   "metadata": {},
   "source": [
    "# XOR neural network\n",
    "\n",
    "This notebook is to serve as a practice for implementing a feed forward neural network and training it with backpropagation algorithm. It is a well known fact that it takes at least two layers to get the network to learn XOR (exclusive or), as the 4 data point for XOR is not linearly separable and on elayer networks are in fact linear classifiers.\n",
    "\n",
    "* Define the weights for each layer and activation ReLU on the hidden layer and sigmoid on the final layer. No fancy implemenation, just basic. \n",
    "\n",
    "* Implememt a feed forward pass with the XOR \"data set\"\n",
    "\n",
    "* Implement backopropagation algorithm\n",
    "\n",
    "* Train the network and see that it gets the XOR correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb473b",
   "metadata": {},
   "source": [
    "### Make a scratch network with 8 units in the first layer and one sigmoid activated ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0b04a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1]\n",
      " [0 1 0 1]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Note that in this setting, the data points are organized in columns instead of rows\n",
    "X = np.array([[0,0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "print(np.vstack((X,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "688c6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each column rrpresents the 3 weights of one of 8 units.\n",
    "W1 = np.random.normal(loc=0.0, scale=1.0, size=(3,8))\n",
    "\n",
    "# and here we have 9 weights corresponding to one last output unit\n",
    "W2 = np.random.normal(loc=0.0, scale=1.0, size=(8,1))\n",
    "\n",
    "def ReLU(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def ReLU_grad(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def loss(guess, y):\n",
    "    return  (- y * np.log(guess) - (1 - y) * np.log(1 - guess)).sum()\n",
    "\n",
    "\n",
    "def loss_grad(guess, y):\n",
    "    return - y * (1/guess) + (1 - y) * (1 / (1 - guess)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179e950",
   "metadata": {},
   "source": [
    "### Make feed forward for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3bc55c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, W1, W2):\n",
    "    results = []\n",
    "    new_X = np.vstack((X, np.ones((1,4))))\n",
    "    z1 = np.matmul(new_X.T, W1)\n",
    "    a1 = np.vectorize(lambda z : ReLu(z))(z1)\n",
    "    z2 = np.matmul(a1, W2)\n",
    "    a2 = np.vectorize(lambda z: sigmoid(z))(z2)\n",
    "    return (z1, a1, z2, a2)\n",
    "    \n",
    "initial_guess = feed_forward(X, W1, W2)[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cedf3",
   "metadata": {},
   "source": [
    "### Implement back propagation with weights update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d29bc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, W1, W2, Y,  loss, loss_grad, step, max_iter = 100):\n",
    "    new_X = np.vstack((X, np.ones((1,4))))\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        z1, a1, z2, a2 = feed_forward(X, W1, W2)\n",
    "        W2 = W2 - step *\\\n",
    "            np.matmul(a1.T, np.matmul(np.diag(np.vectorize(lambda z: sigmoid(z)*(1-sigmoid(z)))(z2).reshape((4,))), loss_grad(a2 , Y.T)))\n",
    "        L = []\n",
    "        for i in range(4): # how to vectorize this?\n",
    "            a = np.diag(np.vectorize(lambda z: ReLU_grad(z))(z1[i,:]))\n",
    "            c = sigmoid_grad(z2[i,:])\n",
    "            L.append(loss_grad(a2[i,:],(Y.T)[i,:])*c*np.matmul(a, W2))\n",
    "\n",
    "        W1 = W1 - step * np.matmul(new_X, np.array(L).reshape(4,8))\n",
    "    return W1, W2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071692a4",
   "metadata": {},
   "source": [
    "### Train the network and see if it get's XOR correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d15b4a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss went from 5.1556868134395195 at random initialization to 0.01640712130851201 after running back propagtion graident descent for 1000 iterations\n",
      "\n",
      "Random initialization gave 2 mistakes. After fitting we have 0 wrongly classified points\n",
      "\n",
      "[[0.00313858 0.        ]\n",
      " [0.9972922  1.        ]\n",
      " [0.99383346 1.        ]\n",
      " [0.00435699 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Fit weights:\n",
    "iterations = 1000\n",
    "A,B = backprop(X, W1, W2, Y,  loss, loss_grad, 0.05, max_iter = iterations)\n",
    "\n",
    "#Make predictions with the fitted weights\n",
    "pred = feed_forward(X, A, B)[-1]\n",
    "\n",
    "\n",
    "print(\"The loss went from {} at random initialization to {} after \\\n",
    "running back propagtion graident descent for {} iterations\".format(loss(initial_guess, Y.T), loss(pred, Y.T), iterations))\n",
    "\n",
    "\n",
    "def check(pred,y):\n",
    "    if (pred<0.5 and y==0) or (pred>=0.5 and y==1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def check_correctness(prediction, Y):\n",
    "    prediction_ = prediction.T.reshape((max(pred.shape),))\n",
    "    Y_ = Y.reshape((max(Y.shape),))\n",
    "    assert prediction_.shape == Y_.shape\n",
    "    L = list(zip(prediction_,Y_))\n",
    "    return  np.vectorize(lambda x: check(x[0], x[1]))(np.array(L,dtype=\"f,f\")).sum()\n",
    "    \n",
    "\n",
    "original_prediction_mistakes = check_correctness(initial_guess, Y)\n",
    "fitted_prediction_mistakes = check_correctness(pred, Y)\n",
    "\n",
    "print()\n",
    "print('Random initialization gave {} mistakes. After fitting we have {} wrongly classified points'\\\n",
    "      .format(original_prediction_mistakes, fitted_prediction_mistakes))\n",
    "\n",
    "print()\n",
    "print(np.hstack((pred, Y.T)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
