{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6c0295",
   "metadata": {},
   "source": [
    "# XOR neural network\n",
    "\n",
    "This notebook is to serve as a practice for implementing a feed forward neural network and training it with backpropagation algorithm. It is a well known fact that it takes at least two layers to get the network to learn XOR (exclusive or), as the 4 data point for XOR is not linearly separable and on elayer networks are in fact linear classifiers.\n",
    "\n",
    "* Define the weights for each layer and activation ReLU on the hidden layer and sigmoid on the final layer. No fancy implemenation, just basic. \n",
    "\n",
    "* Implememt a feed forward pass with the XOR \"data set\"\n",
    "\n",
    "* Implement backopropagation algorithm\n",
    "\n",
    "* Train the network and see that it gets the XOR correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb473b",
   "metadata": {},
   "source": [
    "### Make a scratch network with 8 units in the first layer and one sigmoid activated ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0b04a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1]\n",
      " [0 1 0 1]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Note that in this setting, the data points are organized in columns instead of rows\n",
    "X = np.array([[0,0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "print(np.vstack((X,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "688c6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "   0.95008842 -0.15135721]\n",
      " [-0.10321885  0.4105985   0.14404357  1.45427351  0.76103773  0.12167502\n",
      "   0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574 -2.55298982  0.6536186\n",
      "   0.8644362  -0.74216502]]\n",
      "[[ 1.76405235]\n",
      " [ 0.40015721]\n",
      " [ 0.97873798]\n",
      " [ 2.2408932 ]\n",
      " [ 1.86755799]\n",
      " [-0.97727788]\n",
      " [ 0.95008842]\n",
      " [-0.15135721]]\n"
     ]
    }
   ],
   "source": [
    "# each column rrpresents the 3 weights of one of 8 units.\n",
    "np.random.seed(0)\n",
    "W1 = np.random.normal(loc=0.0, scale=1.0, size=(3,8))\n",
    "np.random.seed(0)\n",
    "# and here we have 9 weights corresponding to one last output unit\n",
    "W2 = np.random.normal(loc=0.0, scale=1.0, size=(8,1))\n",
    "\n",
    "print(W1)\n",
    "print(W2)\n",
    "def ReLU(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def ReLU_grad(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def loss(guess, y):\n",
    "    return  (- y * np.log(guess) - (1 - y) * np.log(1 - guess)).sum()\n",
    "\n",
    "\n",
    "def loss_grad(guess, y):\n",
    "    return - y * (1/guess) + (1 - y) * (1 / (1 - guess)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179e950",
   "metadata": {},
   "source": [
    "### Make feed forward for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3bc55c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 1.49407907, -0.20515826,  0.3130677 , -0.85409574, -2.55298982,\n",
      "         0.6536186 ,  0.8644362 , -0.74216502],\n",
      "       [ 1.39086022,  0.20544024,  0.45711127,  0.60017777, -1.79195209,\n",
      "         0.77529361,  1.30829943, -0.40849069],\n",
      "       [ 3.25813142,  0.19499894,  1.29180569,  1.38679746, -0.68543183,\n",
      "        -0.32365928,  1.81452462, -0.89352223],\n",
      "       [ 3.15491257,  0.60559745,  1.43584926,  2.84107097,  0.0756059 ,\n",
      "        -0.20198427,  2.25838785, -0.5598479 ]]), array([[1.49407907, 0.        , 0.3130677 , 0.        , 0.        ,\n",
      "        0.6536186 , 0.8644362 , 0.        ],\n",
      "       [1.39086022, 0.20544024, 0.45711127, 0.60017777, 0.        ,\n",
      "        0.77529361, 1.30829943, 0.        ],\n",
      "       [3.25813142, 0.19499894, 1.29180569, 1.38679746, 0.        ,\n",
      "        0.        , 1.81452462, 0.        ],\n",
      "       [3.15491257, 0.60559745, 1.43584926, 2.84107097, 0.0756059 ,\n",
      "        0.        , 2.25838785, 0.        ]]), array([[ 3.12456877],\n",
      "       [ 4.81340791],\n",
      "       [11.92150772],\n",
      "       [15.86648845]]), array([[0.95789488],\n",
      "       [0.99194527],\n",
      "       [0.99999335],\n",
      "       [0.99999987]]))\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(X, W1, W2):\n",
    "    results = []\n",
    "    new_X = np.vstack((X, np.ones((1,4))))\n",
    "    z1 = np.matmul(new_X.T, W1)\n",
    "    a1 = np.vectorize(lambda z : ReLu(z))(z1)\n",
    "    z2 = np.matmul(a1, W2)\n",
    "    a2 = np.vectorize(lambda z: sigmoid(z))(z2)\n",
    "    return (z1, a1, z2, a2)\n",
    "    \n",
    "initial_guess = feed_forward(X, W1, W2)[-1]\n",
    "print(feed_forward(X, W1, W2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cedf3",
   "metadata": {},
   "source": [
    "### Implement back propagation with weights update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d29bc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, W1, W2, Y,  loss, loss_grad, step, max_iter = 100):\n",
    "    new_X = np.vstack((X, np.ones((1,4))))\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        z1, a1, z2, a2 = feed_forward(X, W1, W2)\n",
    "        W2 = W2 - step *\\\n",
    "            np.matmul(a1.T, np.matmul(np.diag(np.vectorize(lambda z: sigmoid(z)*(1-sigmoid(z)))(z2).reshape((4,))), loss_grad(a2 , Y.T)))\n",
    "        L = []\n",
    "        for i in range(4): # how to vectorize this?\n",
    "            a = np.diag(np.vectorize(lambda z: ReLU_grad(z))(z1[i,:]))\n",
    "            c = sigmoid_grad(z2[i,:])\n",
    "            L.append(loss_grad(a2[i,:],(Y.T)[i,:])*c*np.matmul(a, W2))\n",
    "\n",
    "        W1 = W1 - step * np.matmul(new_X, np.array(L).reshape(4,8))\n",
    "    return W1, W2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071692a4",
   "metadata": {},
   "source": [
    "### Train the network and see if it get's XOR correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d15b4a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss went from 19.042168579602723 at random initialization to 0.7073466212886892 after running back propagtion graident descent for 1000 iterations\n",
      "\n",
      "Random initialization gave 2 mistakes. After fitting we have 0 wrongly classified points\n",
      "\n",
      "[[0.00413483 0.        ]\n",
      " [0.5        1.        ]\n",
      " [0.99618494 1.        ]\n",
      " [0.00621429 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Fit weights:\n",
    "iterations = 1000\n",
    "A,B = backprop(X, W1, W2, Y,  loss, loss_grad, 0.05, max_iter = iterations)\n",
    "\n",
    "#Make predictions with the fitted weights\n",
    "pred = feed_forward(X, A, B)[-1]\n",
    "\n",
    "\n",
    "print(\"The loss went from {} at random initialization to {} after \\\n",
    "running back propagtion graident descent for {} iterations\".format(loss(initial_guess, Y.T), loss(pred, Y.T), iterations))\n",
    "\n",
    "\n",
    "def check(pred,y):\n",
    "    if (pred<0.5 and y==0) or (pred>=0.5 and y==1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def check_correctness(prediction, Y):\n",
    "    prediction_ = prediction.T.reshape((max(pred.shape),))\n",
    "    Y_ = Y.reshape((max(Y.shape),))\n",
    "    assert prediction_.shape == Y_.shape\n",
    "    L = list(zip(prediction_,Y_))\n",
    "    return  np.vectorize(lambda x: check(x[0], x[1]))(np.array(L,dtype=\"f,f\")).sum()\n",
    "    \n",
    "\n",
    "original_prediction_mistakes = check_correctness(initial_guess, Y)\n",
    "fitted_prediction_mistakes = check_correctness(pred, Y)\n",
    "\n",
    "print()\n",
    "print('Random initialization gave {} mistakes. After fitting we have {} wrongly classified points'\\\n",
    "      .format(original_prediction_mistakes, fitted_prediction_mistakes))\n",
    "\n",
    "print()\n",
    "print(np.hstack((pred, Y.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b935503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
